{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a dummy ML model built from linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30092572 0.51830574 0.98478741]\n",
      " [0.93400601 0.31933276 0.08665472]\n",
      " [0.58195715 0.43230515 0.48727585]] [[0.21062504 0.58422818]\n",
      " [0.24512291 0.16850848]\n",
      " [0.20312982 0.78691849]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(3,3)\n",
    "y = np.random.rand(3,2)\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7243358 , -8.09688886])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "model.predict(np.array([1,2,3]).reshape(1,-1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile = open('trained_model', 'wb')\n",
    "#pickle the object and store it in a file\n",
    "pickle.dump(model, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7243358 , -8.09688886]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that the object is correctly pickled and works when unpickled\n",
    "del model\n",
    "picklefile = open('trained_model', 'rb')\n",
    "new_model = pickle.load(picklefile)\n",
    "new_model.predict(np.array([1,2,3]).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Pioneers Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'train_pricing_decisions' is a CSV file in the 'data' folder\n",
    "train_pricing_decisions = pd.read_csv('train_prices_decisions_2024.csv')\n",
    "# Split the data into training and validation sets (70-30 split)\n",
    "train_data, val_data = train_test_split(train_pricing_decisions, test_size=0.3, random_state=42)\n",
    "# Import the actual testing dataset\n",
    "test_user_info = pd.read_csv('test_user_info_2024.csv')\n",
    "\n",
    "#Creating prices_to_predict array based on min and max prices in train_pricing_decisions\n",
    "print('min: ', np.round(train_pricing_decisions.price_item.min(), 2), \n",
    "      'max: ', np.round(train_pricing_decisions.price_item.max(), 2))\n",
    "prices_to_predict = np.arange(train_pricing_decisions.price_item.min(),  train_pricing_decisions.price_item.max(), 4)\n",
    "\n",
    "\n",
    "min_price = train_pricing_decisions.price_item.min()\n",
    "max_price = train_pricing_decisions.price_item.max()\n",
    "\n",
    "# Generate more points near the center\n",
    "n_center = 120  # Number of central points\n",
    "center_points = np.linspace(min_price + (max_price - min_price) * 0.1,  # Start 10% above min\n",
    "                             max_price - (max_price - min_price) * 0.1,  # End 10% below max\n",
    "                             n_center)\n",
    "\n",
    "# Generate fewer points near the edges\n",
    "n_edges = 20  # Number of edge points\n",
    "edge_points = np.concatenate([\n",
    "    np.linspace(min_price, min_price + (max_price - min_price) * 0.1, n_edges // 2),\n",
    "    np.linspace(max_price - (max_price - min_price) * 0.1, max_price, n_edges // 2)\n",
    "])\n",
    "\n",
    "# Combine and sort the points\n",
    "prices_to_predict = np.sort(np.concatenate([center_points, edge_points]))\n",
    "\n",
    "print(prices_to_predict)\n",
    "\n",
    "print(min(prices_to_predict), max(prices_to_predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the users using K-mean to segment the customer base\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cluster the users using K-means to segment the customer base\n",
    "def cluster(data, n_clusters):\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering on the covariates in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing covariates.\n",
    "        n_clusters (int): Number of clusters to form. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A pandas Series containing the cluster labels for each row in the data.\n",
    "    \"\"\"\n",
    "    # Extract the covariate columns for clustering\n",
    "    covariates = data[['Covariate1', 'Covariate2', 'Covariate3']]\n",
    "    \n",
    "    # Standardize the covariates to ensure equal contribution of features\n",
    "    scaler = StandardScaler()\n",
    "    covariates_scaled = scaler.fit_transform(covariates)\n",
    "    \n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(covariates_scaled)\n",
    "    \n",
    "    # Add the cluster labels back to the original DataFrame\n",
    "    data['cluster'] = cluster_labels\n",
    "    \n",
    "    return data, kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Pick the optimal cluster number by using elbow method on the training set\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(train_data[['Covariate1', 'Covariate2', 'Covariate3']])\n",
    "\n",
    "# Compute inertia for different numbers of clusters\n",
    "inertias = []\n",
    "cluster_range = range(1, 11)  # Test k from 1 to 10\n",
    "\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cluster_range, inertias, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-mean clustering using K=4 for customer segmentation ***** USING ENTIRE DATASET, TRAIN + VALIDATION *****\n",
    "segmented_train, kmeans_model = cluster(train_pricing_decisions, 4)\n",
    "segmented_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating logistic regression function to predict demand based on training_pricing_decisions and prices_to_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def fit_logistic_regression_demand_with_covariates(df):\n",
    "    model = LogisticRegression(fit_intercept=True)\n",
    "    X = df[['price_item', 'Covariate1', 'Covariate2', 'Covariate3']]\n",
    "    y = df['item_bought'].astype(int)  \n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# model = fit_logistic_regression_demand_with_covariates(train_pricing_decisions)\n",
    "\n",
    "def get_prediction_logistic(fitted_model, price, covariates):\n",
    "    input_data = pd.DataFrame({\n",
    "        'price_item': [price],     \n",
    "        'Covariate1': [covariates[0]], \n",
    "        'Covariate2': [covariates[1]],\n",
    "        'Covariate3': [covariates[2]]\n",
    "    })\n",
    "    prediction = fitted_model.predict_proba(input_data)[:, 1]  \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store results for each cluster\n",
    "models = []\n",
    "\n",
    "# Loop through unique cluster values\n",
    "for cluster in range(4):\n",
    "    # Filter the DataFrame for the current cluster\n",
    "    cluster_df = segmented_train[segmented_train['cluster'] == cluster]\n",
    "    \n",
    "    model = fit_logistic_regression_demand_with_covariates(cluster_df)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating Cluster Assigments with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# the function takes in dataframe of covariates(df), list of prices(prices_to_predict), and list of segmented logistic regression models(cluster_log_models)\n",
    "#  --> outputs f1 score\n",
    "def get_f1(df, cluster_log_models, typ, kmeans_model=kmeans_model):\n",
    "    # Getting predictions for train_pricing_decision to get average price\n",
    "    buy_predictions = []\n",
    "    for row in df.itertuples(index=False, name='Pandas'):\n",
    "        # Extract the relevant covariates using their field names\n",
    "        covariates = [row.Covariate1, row.Covariate2, row.Covariate3]\n",
    "        price = row.price_item\n",
    "        assigned_cluster = kmeans_model.predict([covariates])[0]\n",
    "    \n",
    "        input_data = pd.DataFrame({\n",
    "        'price_item': [price],     \n",
    "        'Covariate1': [covariates[0]], \n",
    "        'Covariate2': [covariates[1]],\n",
    "        'Covariate3': [covariates[2]]\n",
    "        })\n",
    "        buy_pred = cluster_log_models[assigned_cluster].predict(input_data)\n",
    "        buy_predictions.append(buy_pred)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(df[\"item_bought\"], buy_predictions, average='weighted')  # Use 'weighted' for imbalanced datasets\n",
    "    print(f\"F1 Score {typ}: {f1}\")\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get f1 for validation dataset\n",
    "f1_val = get_f1(val_data, models, \"validation\", kmeans_model=kmeans_model)\n",
    "f1_train = get_f1(train_data, models, \"train\", kmeans_model=kmeans_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Demand Predictions, using our clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function takes in dataframe of covariates(df), list of prices(prices_to_predict), and list of segmented logistic regression models(cluster_log_models)\n",
    "#  --> outputs demand predictions\n",
    "def get_demand_predictions_clusters(df, prices_to_predict, cluster_log_models, kmeans_model=kmeans_model):\n",
    "    # Getting predictions for train_pricing_decision to get average price\n",
    "    demand_predictions = []\n",
    "    i = 0\n",
    "    for row in df.itertuples(index=False, name='Pandas'):\n",
    "        # Extract the relevant covariates using their field names\n",
    "        covariates = [[row.Covariate1, row.Covariate2, row.Covariate3]]\n",
    "        assigned_cluster = kmeans_model.predict(covariates)[0]\n",
    "\n",
    "        demand_prediction = []\n",
    "        for price in prices_to_predict:\n",
    "            demand_prediction.append(get_prediction_logistic(cluster_log_models[assigned_cluster], price, [row.Covariate1, row.Covariate2, row.Covariate3]))\n",
    "        i += 1\n",
    "        if i % 5000 == 0:\n",
    "            print(i)    \n",
    "        \n",
    "        demand_predictions.append(demand_prediction)\n",
    "    \n",
    "    return demand_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demand_prediction_clusters(df, prices_to_predict, cluster_log_models, kmeans_model=kmeans_model):    \n",
    "    covariates = [[df.Covariate1, df.Covariate2, df.Covariate3]]\n",
    "    \n",
    "    assigned_cluster = kmeans_model.predict(covariates)[0]\n",
    "    demand_prediction = []\n",
    "    for price in prices_to_predict:\n",
    "        demand_prediction.append(get_prediction_logistic(cluster_log_models[assigned_cluster], price, [df.Covariate1, df.Covariate2, df.Covariate3]))\n",
    "\n",
    "    return demand_prediction    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get demand predictions for training dataset\n",
    "train_demand_predictions = get_demand_prediction_clusters(train_pricing_decisions.iloc[1000,:], prices_to_predict, models, kmeans_model=kmeans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(prices_to_predict, train_demand_predictions);\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Probability of Purchase\")\n",
    "plt.title(\"Price vs Purchase Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_program(prices_to_predict, demand_prediction, T, K):\n",
    "    demand_pred = np.array(demand_prediction)\n",
    "    ratio = K/T\n",
    "    if ratio>=0.9:\n",
    "        ratio = 0.9\n",
    "    diff = np.abs(demand_pred - ratio)\n",
    "    return prices_to_predict[np.argmin(diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_step_revenue_maximizing_price_and_revenue_k(Vtplus1k, Vtplus1kminus1, price_options, demand_predictions):\n",
    "    rev_list = (np.array(price_options)+Vtplus1kminus1*np.ones(len(price_options)))*np.array(demand_predictions)+(np.ones(len(demand_predictions))-demand_predictions)*Vtplus1k\n",
    "    opt_index = np.argmax(rev_list)\n",
    "    Ptk = price_options[opt_index]\n",
    "    # print(len(rev_list), type(rev_list), opt_index)\n",
    "    # print(len(price_options), type(price_options), opt_index)\n",
    "    vtk = rev_list[opt_index]\n",
    "    return Ptk, vtk\n",
    "\n",
    "def get_prices_over_time_and_expected_revenue_k(prices, demand_predictions, T, K):\n",
    "    opt_price_list=np.zeros([T,K+1])\n",
    "    V = np.zeros([T+1,K+1])\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        for k in range(1, K + 1):  # We cannot sell if k = 0\n",
    "            # Optimize the price given the future value function\n",
    "            V_t_k = V[t + 1][k]\n",
    "            V_t_k_minus_1 = V[t + 1][k - 1] if k > 0 else None\n",
    "            opt_price, max_value = get_single_step_revenue_maximizing_price_and_revenue_k(V_t_k, V_t_k_minus_1, prices, demand_predictions)\n",
    "            V[t][k] = max_value  # Update the value function\n",
    "            opt_price_list[t][k] = opt_price  # Store the optimal price for time t and k items left   \n",
    "    return opt_price_list, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create thresholds based off of segmented training data (1 for each cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal training prices for all t<20, k<=12\n",
    "opt_prices = []\n",
    "for user in train_demand_predictions:\n",
    "    opt_prices.append(get_prices_over_time_and_expected_revenue_k(prices_to_predict, user, T=20, K=12)[0])\n",
    "\n",
    "# Getting rid of the 0 s.\n",
    "training_opt_prices = np.array(opt_prices)[:,:,1:]\n",
    "\n",
    "# Calculating the average to charge the people with less willingness to pay\n",
    "threshold_avg = pd.DataFrame(np.average(training_opt_prices, axis=0)).T\n",
    "\n",
    "# The cutoff for people that has low willingness to pay using the 1th percentile of the training data across all the users\n",
    "threshold_matrix_1percentile = []\n",
    "for k in training_opt_prices.T:\n",
    "    threshold_list = []\n",
    "    for t in k:\n",
    "        threshold_list.append(np.percentile(t, 1))\n",
    "    threshold_matrix_1percentile.append(threshold_list)\n",
    "threshold_1percentile = pd.DataFrame(threshold_matrix_1percentile)\n",
    "# threshold_1percentile[19] = 0\n",
    "k = threshold_1percentile.shape[0] # number of inventory\n",
    "t = threshold_1percentile.shape[1] # number of customers\n",
    "\n",
    "# Create a mask with the same shape as the DataFrame\n",
    "mask = np.zeros((k, t), dtype=bool)\n",
    "\n",
    "# Populate the mask where row index <= column index\n",
    "for i in range(k):\n",
    "    for j in range(t):\n",
    "        if i <= j - 8:\n",
    "            mask[i, j] = True\n",
    "\n",
    "# Set the selected entries to 0\n",
    "threshold_1percentile[mask] = 0\n",
    "\n",
    "threshold_1percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure train_demand_predictions is converted to a DataFrame with headers from prices_to_predict\n",
    "train_demand_predictions_df = pd.DataFrame(train_demand_predictions, columns=prices_to_predict)\n",
    "print(train_demand_predictions_df.shape)\n",
    "# Concatenate the DataFrame with segmented_train\n",
    "# segmented_train = pd.concat([segmented_train.reset_index(drop=True), train_demand_predictions_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Verify the result\n",
    "print(segmented_train.shape)  # Should have 50,000 rows and original + new columns\n",
    "# print(segmented_train.head())  # Display the first few rows\n",
    "\n",
    "\n",
    "# Separate the data by cluster and calculate thresholds for each cluster\n",
    "cluster_thresholds = {}  # Dictionary to store threshold dataframes for each cluster\n",
    "cluster_replacements = {}  # Dictionary to store replacment dataframes for each cluster (if customer below threshold, give them this)\n",
    "\n",
    "for cluster_id in [0, 1, 2, 3]:\n",
    "    # Filter the users in the current cluster\n",
    "    cluster_users = segmented_train[segmented_train['cluster'] == cluster_id]\n",
    "\n",
    "    # Optimal training prices for all t<20, k<=12 for the current cluster\n",
    "    opt_prices = []\n",
    "    for _, user in cluster_users.iterrows():  # Iterate over rows\n",
    "        # Create a copy of the user row excluding the first 7 columns\n",
    "        user_data = user.iloc[7:].copy()\n",
    "        # print(len(np.array(user_data)))\n",
    "        # print(np.array(user_data))\n",
    "        # print(len(prices_to_predict))\n",
    "        # print(prices_to_predict)\n",
    "        \n",
    "        # Use this user data to compute optimal prices\n",
    "        opt_prices.append(get_prices_over_time_and_expected_revenue_k(prices_to_predict, np.array(user_data), T=20, K=12)[0])\n",
    "\n",
    "    # Getting rid of the 0s\n",
    "    training_opt_prices = np.array(opt_prices)[:,:,1:]\n",
    "\n",
    "    # Calculating the average to charge people with less willingness to pay\n",
    "    threshold_avg = pd.DataFrame(np.average(training_opt_prices, axis=0)).T\n",
    "\n",
    "    # The cutoff for people with low willingness to pay using the 10th percentile of the training data across all users in the cluster\n",
    "    threshold_matrix_1percentile = []\n",
    "    replacment_matrix_65percentile = []\n",
    "\n",
    "    for k in training_opt_prices.T:\n",
    "        threshold_list = []\n",
    "        replacement_list = []\n",
    "        for t in k:\n",
    "            threshold_list.append(np.percentile(t, 1))\n",
    "            replacement_list.append(np.percentile(t, 65))\n",
    "        threshold_matrix_1percentile.append(threshold_list)\n",
    "        replacment_matrix_65percentile.append(replacement_list)\n",
    "\n",
    "    threshold_1percentile = pd.DataFrame(threshold_matrix_1percentile)\n",
    "    k = threshold_1percentile.shape[0] # number of inventory\n",
    "    t = threshold_1percentile.shape[1] # number of customers\n",
    "\n",
    "    # Create a mask with the same shape as the DataFrame\n",
    "    mask = np.zeros((k, t), dtype=bool)\n",
    "\n",
    "    # Populate the mask where row index <= column index\n",
    "    for i in range(k):\n",
    "        for j in range(t):\n",
    "            if i <= j - 8:\n",
    "                mask[i, j] = True\n",
    "\n",
    "    # Set the selected entries to 0\n",
    "    threshold_1percentile[mask] = 0\n",
    "\n",
    "    # k<=t\n",
    "    # threshold_10percentile[19] = 0\n",
    "    # threshold_10percentile[18] = 0\n",
    "\n",
    "    replacment_65percentile = pd.DataFrame(replacment_matrix_65percentile)\n",
    "\n",
    "    # Store the threshold dataframe for the current cluster\n",
    "    cluster_thresholds[cluster_id] = threshold_1percentile\n",
    "    cluster_replacements[cluster_id] = replacment_65percentile\n",
    "\n",
    "# Access the threshold and replacement dataframes for each cluster\n",
    "for cluster_id in [0, 1, 2, 3]:\n",
    "    threshold_df = cluster_thresholds[cluster_id]\n",
    "    replacement_df = cluster_replacements[cluster_id]\n",
    "    print(f\"Threshold DataFrame for Cluster {cluster_id}:\\n\", threshold_df)\n",
    "    print(f\"Replacement DataFrame for Cluster {cluster_id}:\\n\", replacement_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_1percentile.to_csv('threshold_1percentile.csv')\n",
    "\n",
    "# threshold_avg.to_csv('threshold_avg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_thresholds[0].to_csv('threshold_10percentile0.csv')\n",
    "cluster_replacements[0].to_csv('threshold_replacement0.csv')\n",
    "\n",
    "cluster_thresholds[1].to_csv('threshold_10percentile1.csv')\n",
    "cluster_replacements[1].to_csv('threshold_replacement1.csv')\n",
    "\n",
    "cluster_thresholds[2].to_csv('threshold_10percentile2.csv')\n",
    "cluster_replacements[2].to_csv('threshold_replacement2.csv')\n",
    "\n",
    "cluster_thresholds[3].to_csv('threshold_10percentile3.csv')\n",
    "cluster_replacements[3].to_csv('threshold_replacement3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_func(opt_price, t, k, threshold, replacement):\n",
    "    if opt_price < threshold.iloc[k-1, 20-t]:\n",
    "        return replacement.iloc[k-1, 20-t]\n",
    "    else:\n",
    "        return opt_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serialized our logistic regression models that predicts demand probabilities across many prices (prices_to_predict) for 4 different clusters\n",
    "import pickle\n",
    "\n",
    "with open('demand_logistic_reg_kmeans_0.pkl', 'wb') as f:\n",
    "    pickle.dump(models[0], f)\n",
    "with open('demand_logistic_reg_kmeans_1.pkl', 'wb') as f:\n",
    "    pickle.dump(models[1], f)\n",
    "with open('demand_logistic_reg_kmeans_2.pkl', 'wb') as f:\n",
    "    pickle.dump(models[2], f)\n",
    "with open('demand_logistic_reg_kmeans_3.pkl', 'wb') as f:\n",
    "    pickle.dump(models[3], f)\n",
    "\n",
    "# pickle kmeans file\n",
    "with open('kmeans.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans_model, f)\n",
    "\n",
    "# with open('demand_logistic_reg.pkl', 'rb') as f:\n",
    "#     loaded_model = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''  This function is called every time the agent needs to choose an action by the environment.\n",
    "\n",
    "The input 'obs' is a 5 tuple, containing the following information:\n",
    "-- new_buyer_covariates: a vector of length 3, containing the covariates of the new buyer.\n",
    "-- last_sale: a tuple of length 2. The first element is the index of the agent that made the last sale, if it is NaN, then the customer did not make a purchase. \n",
    "    The second element is a numpy array of length n_agents, containing the prices that were offered by each agent in the last sale.\n",
    "-- state: a vector of length n_agents, containing the current profit of each agent.\n",
    "-- inventories: a vector of length n_agents, containing the current inventory level of each agent.\n",
    "-- time_until_replenish: an integer indicating the time until the next replenishment, by which time your (and your opponent's, in part 2) remaining inventory will be reset to the inventory limit.\n",
    "\n",
    "The expected output is a single number, indicating the price that you would post for the new buyer.\n",
    "      \n",
    "'''\n",
    "\n",
    "def get_single_user_price(new_buyer_covariates, last_sale, state, inventories, time_until_replenish):\n",
    "    with open('demand_logistic_reg.pkl', 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    demand_prediction = []\n",
    "    for price in prices_to_predict:\n",
    "        demand_prediction.append(get_prediction_logistic(loaded_model, price, new_buyer_covariates))\n",
    "    opt_price =  get_prices_over_time_and_expected_revenue_k(prices_to_predict, demand_prediction, T=20-time_until_replenish, K=inventories)[0][0][-1]\n",
    "    opt_price = threshold_func(opt_price, 20-time_until_replenish, K=inventories)\n",
    "    return opt_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(training_opt_prices[training_opt_prices<=150].flatten(), bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing final function with inputs from pricepioneers.py/agent function\n",
    "get_single_user_price([0.3, 0.4, 0.5], last_sale=0, state=0, inventories=12, time_until_replenish=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Static CSV of predicted price for test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Getting demand prdictions for test_user_info to feed into the bellman equations\n",
    "# # demand_predictions: Each row represents a user and each column represents a price\n",
    "# demand_predictions = []\n",
    "# for row in test_user_info.itertuples(index=False, name='Pandas'):\n",
    "#     demand_prediction = []\n",
    "#     for price in prices_to_predict:\n",
    "#         demand_prediction.append(get_prediction_logistic(model, price, [row.Covariate1, row.Covariate2, row.Covariate3]))\n",
    "#     demand_predictions.append(demand_prediction)\n",
    "# print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get demand predictions for test dataset\n",
    "test_demand_predictions = get_demand_predictions_clusters(test_user_info, prices_to_predict, models, kmeans_model=kmeans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_single_step_revenue_maximizing_price_and_revenue(Vtplus1, price_options, demand_predictions):\n",
    "    max_value = float('-inf')\n",
    "    optimal_price = None\n",
    "    for price, demand in zip(price_options, demand_predictions):\n",
    "        expected_revenue = demand * price + (1 - demand) * Vtplus1\n",
    "\n",
    "        if expected_revenue > max_value:\n",
    "            max_value = expected_revenue\n",
    "            optimal_price = price\n",
    "\n",
    "    return optimal_price*0.95, max_value*0.95\n",
    "\n",
    "\n",
    "optimal_prices = []\n",
    "optimal_rev = []\n",
    "for user in test_demand_predictions:\n",
    "    price, rev = get_single_step_revenue_maximizing_price_and_revenue(0, prices_to_predict, user)\n",
    "    optimal_prices.append(price)\n",
    "    optimal_rev.append(rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check\n",
    "len(optimal_rev) == len(optimal_prices) == len(test_user_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the optimal prices into csv for test_user_info\n",
    "dict = {'user_index': test_user_info.user_index, 'price_item': optimal_prices, 'expected_revenue':optimal_rev}\n",
    "df = pd.DataFrame(dict)\n",
    "df.to_csv('static_prices_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "import matplotlib.pyplot as plt\n",
    "# Plot each row as a separate line\n",
    "for i in range(data.shape[0]):\n",
    "    plt.plot(data[i, :], marker='o', label=f'Row {i+1}')  # Plot each row as a line\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('T')\n",
    "plt.ylabel('Optimal Price')\n",
    "plt.title('Line Plot of Rows in a 5x10 NumPy Array')\n",
    "\n",
    "# Add legend to identify each line\n",
    "plt.legend()\n",
    "\n",
    "# Display the grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
